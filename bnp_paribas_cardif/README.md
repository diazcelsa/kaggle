As a global specialist in personal insurance, BNP Paribas Cardif serves 90 million clients in 36 countries across Europe, Asia and Latin America.

In a world shaped by the emergence of new uses and lifestyles, everything is going faster and faster. When facing unexpected events, customers expect their insurer to support them as soon as possible. However, claims management may require different levels of check before a claim can be approved and a payment can be made. With the new practices and behaviors generated by the digital economy, this process needs adaptation thanks to data science to meet the new needs and expectations of customers.
In this challenge, BNP Paribas Cardif is providing an anonymized database with two categories of claims:

    1. claims for which approval could be accelerated leading to faster payments
    2. claims for which additional information is required before approval

Kagglers are challenged to predict the category of a claim based on features available early in the process, helping BNP Paribas Cardif accelerate its claims process and therefore provide a better service to its customers.

You are provided with an anonymized dataset containing both categorical and numeric variables available when the claims were received by BNP Paribas Cardif. All string type variables are categorical. There are no ordinal variables.

The "target" column in the train set is the variable to predict. It is equal to 1 for claims suitable for an accelerated approval.

The task is to predict a probability ("PredictedProb") for each claim in the test set.

File descriptions:

train.csv - the training set including the target
test.csv - the test set without the target
sample_submission.csv - a sample submission file in the correct format

The analysis of the data and the prediction of the model is performed in the files and notebooks:

    - check_input_data.ipynb
        - Explore the format of data given and the format expected for the submission
        - Explore the different data types in the features and when continous, if they follow a normal distribution or not
        - Explore wether or not there are missing values
        - Explore how to preprocess the data for a later prediction

    - data_modifier.py
        - Classes and Methods to process the datasets

    - gen_pipeline.ipynb
        - Develope a protocol for processing data in order to learn out of it
        - Develope new classes and methods (data_modifier.py) and evaluate them to process data
        - Explore sklearn classes and methods to process our data
        - Explore how to organize the steps to include them into the pipeline

    - cv_roc.ipynb
        - Generate a train and a test sets out of the initial given train data
        - Apply the protocol to process train and test sets in a pipeline
        - Generate a model using processed data with Random Forest classifier and default parameters
        - Apply 3-fold Cross Validation to train dataset and perform the predictions in each case
        - Evaluate the models generated by plotting the ROC Curve and comparing the Area under the curve for each dataset

    - gs_roc.ipynb
        - Generate and evaluate predictions through Grid Search
        - Compare different values for different parameters of Random Forest classifier

    - run_predict.py
        - Run final prediction with test data

    - results.csv
        - Final results
